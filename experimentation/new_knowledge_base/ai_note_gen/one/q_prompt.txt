##Instructions:
You will be given a student's course outline, along with the specific module the student is currently working on.
Your goal is to generate course notes ONLY ON THE CURRENT MODULE the student is working on.
The goal of the course notes will be to help the student develop a strong understanding of that topic.

Your course notes must be generated such that it is specific for the student, given their goals and background.
The notes should be generated such that, it is very engaging and easy for the student to understand the material.
Please ensure your course notes for the module are as DETAILED AS POSSIBLE. The more detail, the better for the student.

DO NOT GENERATE ANY NOTES FOR FUTURE MODULES. ONLY FOCUS ON THE CURRENT MODULE.
As mentioned below, to help you generate detailed notes, you will be given additional, relevant supplementary material from different textbooks and wikipedia.
Please use this supplementary material for your course note generation.
Please note that your course notes will be the primary resource for the student, so ensure they are very detailed, rich with examples.

Below, you will be given 4 critical pieces of information:
- The student's goals, what they want to learn, and their background.
- The outline of the course the student is currently taking.
- The specific topic to focus your course notes on.
- Additional supplementary material on that topic, which you can leverage to help you generate the course notes.

Your response MUST BE OUTPUTED IN JSON FORMAT, containing the following key:
- "course_notes"
    - This will be the course notes IN MARKDOWN FORMAT, which will be presented to the student.
    - Please ensure at the beginning of your markdown, you include the Current Module Name and SubTopics that will be covered, before you include your notes.

##Student Goals/Background Information
The student wants to learn the foundations of AI, specifically Machine Learning and Deep Learning. They also want to apply this knowledge to real-world applications using LLMs such as GPT.

##Course Outline
Course Name:
Introduction to Artificial Intelligence and Machine Learning

Course Description:
Module 1: Introduction to Artificial Intelligence
- What is Artificial Intelligence?
- Overview of AI applications
- Ethics and impact of AI

Module 2: Fundamentals of Machine Learning
- Introduction to machine learning
- Supervised, unsupervised, and reinforcement learning
- Training and testing models

Module 3: Neural Networks and Deep Learning
- Understanding neural networks
- Deep learning concepts
- Building and training neural networks

Module 4: AI Applications
- Real-world applications of AI
- Case studies
- Project planning

Module 5: Large Language Models and GPT
- Overview of large language models like GPT
- Understanding GPT-3
- Practical applications of GPT

Module 6: Building AI Applications with GPT API
- Introduction to GPT API
- Implementing GPT API in projects
- Developing a simple application using GPT API


##Current Week Topic Information
Module 1: Introduction to Artificial Intelligence
- What is Artificial Intelligence?

##Supplementary Material
Attention? Attention! | Lil'Log

4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 1/25

Posts Archive Search Tags FAQ emojisearch.app

Attention? Attention!
Date: June 24, 2018 | Estimated Reading Time: 21 min | Author: Lilian Weng

Table of Contents

[Updated on 2018-10-28: Add Pointer Network and the link to my implementation of

Transformer.]

[Updated on 2018-11-06: Add a link to the implementation of Transformer model.]

[Updated on 2018-11-18: Add Neural Turing Machines.]

[Updated on 2019-07-18: Correct the mistake on using the term “self-attention” when

introducing the show-attention-tell paper; moved it to Self-Attention section.]

[Updated on 2020-04-07: A follow-up post on improved Transformer models is here.]

Attention is, to some extent, motivated by how we pay visual attention to different regions of an

image or correlate words in one sentence. Take the picture of a Shiba Inu in Fig. 1 as an

example.

Fig. 1. A Shiba Inu in a men s̓ outfit. The credit of the original photo goes to
Instagram @mensweardog.

https://lilianweng.github.io/
https://lilianweng.github.io/archives
https://lilianweng.github.io/search/
https://lilianweng.github.io/tags/
https://lilianweng.github.io/faq
https://www.emojisearch.app/
https://github.com/lilianweng/transformer-tensorflow
https://github.com/lilianweng/transformer-tensorflow
https://arxiv.org/abs/1502.03044
https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/
https://www.instagram.com/mensweardog/?hl=en


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 2/25

Human visual attention allows us to focus on a certain region with “high resolution” (i.e. look at

the pointy ear in the yellow box) while perceiving the surrounding image in “low resolution” (i.e.

now how about the snowy background and the outfit?), and then adjust the focal point or do the

inference accordingly. Given a small patch of an image, pixels in the rest provide clues what

should be displayed there. We expect to see a pointy ear in the yellow box because we have

seen a dog s̓ nose, another pointy ear on the right, and Shiba s̓ mystery eyes (stuff in the red

boxes). However, the sweater and blanket at the bottom would not be as helpful as those doggy

features.

Similarly, we can explain the relationship between words in one sentence or close context.

When we see “eating”, we expect to encounter a food word very soon. The color term describes

the food, but probably not so much with “eating” directly.

Fig. 2. One word "attends" to other words in the same sentence differently.

In a nutshell, attention in deep learning can be broadly interpreted as a vector of importance

weights: in order to predict or infer one element, such as a pixel in an image or a word in a

sentence, we estimate using the attention vector how strongly it is correlated with (or “attends

to” as you may have read in many papers) other elements and take the sum of their values

weighted by the attention vector as the approximation of the target.

Whatʼs Wrong with Seq2Seq Model?

The seq2seq model was born in the field of language modeling (Sutskever, et al. 2014). Broadly

speaking, it aims to transform an input sequence (source) to a new one (target) and both

sequences can be of arbitrary lengths. Examples of transformation tasks include machine

translation between multiple languages in either text or audio, question-answer dialog

generation, or even parsing sentences into grammar trees.

The seq2seq model normally has an encoder-decoder architecture, composed of:

An encoder processes the input sequence and compresses the information into a context

vector (also known as sentence embedding or “thought” vector) of a fixed length. This

https://arxiv.org/abs/1409.3215


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 3/25

representation is expected to be a good summary of the meaning of the whole source

sequence.

A decoder is initialized with the context vector to emit the transformed output. The early

work only used the last state of the encoder network as the decoder initial state.

Both the encoder and decoder are recurrent neural networks, i.e. using LSTM or GRU units.

Fig. 3. The encoder-decoder model, translating the sentence "she is eating
a green apple" to Chinese. The visualization of both encoder and decoder is

unrolled in time.

A critical and apparent disadvantage of this fixed-length context vector design is incapability of

remembering long sentences. Often it has forgotten the first part once it completes processing

the whole input. The attention mechanism was born (Bahdanau et al., 2015) to resolve this

problem.

Born for Translation

The attention mechanism was born to help memorize long source sentences in neural machine

translation (NMT). Rather than building a single context vector out of the encoder s̓ last hidden

state, the secret sauce invented by attention is to create shortcuts between the context vector

and the entire source input. The weights of these shortcut connections are customizable for

each output element.

While the context vector has access to the entire input sequence, we donʼt need to worry about

forgetting. The alignment between the source and target is learned and controlled by the

context vector. Essentially the context vector consumes three pieces of information:

encoder hidden states;

http://colah.github.io/posts/2015-08-Understanding-LSTMs/
https://arxiv.org/pdf/1409.0473.pdf
https://arxiv.org/pdf/1409.0473.pdf


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 4/25

decoder hidden states;

alignment between source and target.

Fig. 4. The encoder-decoder model with additive attention mechanism in
Bahdanau et al., 2015.

Definition

Now let s̓ define the attention mechanism introduced in NMT in a scientific way. Say, we have a

source sequence  of length  and try to output a target sequence  of length :

(Variables in bold indicate that they are vectors; same for everything else in this post.)

The encoder is a bidirectional RNN (or other recurrent network setting of your choice) with a

forward hidden state  and a backward one . A simple concatenation of two represents the

encoder state. The motivation is to include both the preceding and following words in the

annotation of one word.

x n y m

x = [x1,x2, … ,xn]
y = [y1, y2, … , ym]

→
hi

←
h i

hi = [
→
h

⊤
i ;

←
h

⊤

i ]⊤, i = 1, … ,n

https://arxiv.org/pdf/1409.0473.pdf
https://www.coursera.org/lecture/nlp-sequence-models/bidirectional-rnn-fyXnn


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 5/25

The decoder network has hidden state  for the output word at position t,

, where the context vector  is a sum of hidden states of the input sequence,

weighted by alignment scores:

The alignment model assigns a score  to the pair of input at position i and output at position

t, , based on how well they match. The set of  are weights defining how much of

each source hidden state should be considered for each output. In Bahdanau s̓ paper, the

alignment score  is parametrized by a feed-forward network with a single hidden layer and

this network is jointly trained with other parts of the model. The score function is therefore in

the following form, given that tanh is used as the non-linear activation function:

where both  and  are weight matrices to be learned in the alignment model.

The matrix of alignment scores is a nice byproduct to explicitly show the correlation between

source and target words.

st = f(st−1, yt−1, ct)

t = 1, … ,m ct

ct =
n

∑
i=1

αt,ihi ; Context vector for output yt

αt,i = align(yt,xi) ; How well two words yt and xi are aligned.

=
exp(score(st−1, hi))

∑n
i′=1 exp(score(st−1, hi′))

; Softmax of some predefined alignment score..

αt,i

(yt,xi) {αt,i}

α

score(st, hi) = v⊤
a tanh(Wa[st; hi])

va Wa



4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 6/25

Fig. 5. Alignment matrix of "L'accord sur l'Espace économique européen a
été signé en août 1992" (French) and its English translation "The agreement
on the European Economic Area was signed in August 1992". (Image

source: Fig 3 in Bahdanau et al., 2015)

Check out this nice tutorial by Tensorflow team for more implementation instructions.

A Family of Attention Mechanisms

With the help of the attention, the dependencies between source and target sequences are not

restricted by the in-between distance anymore! Given the big improvement by attention in

machine translation, it soon got extended into the computer vision field (Xu et al. 2015) and

people started exploring various other forms of attention mechanisms (Luong, et al., 2015; Britz

et al., 2017; Vaswani, et al., 2017).

Summary

Below is a summary table of several popular attention mechanisms and corresponding

alignment score functions:

Name Alignment score function Citation

Content-base Graves2014score(st, hi) = cosine[st, hi]

https://arxiv.org/pdf/1409.0473.pdf
https://www.tensorflow.org/versions/master/tutorials/seq2seq
http://proceedings.mlr.press/v37/xuc15.pdf
https://arxiv.org/pdf/1508.04025.pdf
https://arxiv.org/abs/1703.03906
https://arxiv.org/abs/1703.03906
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
https://arxiv.org/abs/1410.5401


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 7/25

Name Alignment score function Citation

attention

Additive(*) Bahdanau2015

Location-
Base Note: This simplifies the softmax alignment to only depend on

the target position.
Luong2015

General
where  is a trainable weight matrix in the attention layer.

Luong2015

Dot-Product Luong2015

Scaled Dot-
Product(^)

Note: very similar to the dot-product attention except for a
scaling factor; where n is the dimension of the source hidden
state.

Vaswani2017

(*) Referred to as “concat” in Luong, et al., 2015 and as “additive attention” in Vaswani, et al.,

2017.

(^) It adds a scaling factor , motivated by the concern when the input is large, the

softmax function may have an extremely small gradient, hard for efficient learning.

Here are a summary of broader categories of attention mechanisms:

Name Definition Citation

Self-
Attention(&)

Relating different positions of the same input sequence.
Theoretically the self-attention can adopt any score functions
above, but just replace the target sequence with the same input
sequence.

Cheng2016

Global/Soft Attending to the entire input state space. Xu2015

Local/Hard
Attending to the part of input state space; i.e. a patch of the
input image.

Xu2015;
Luong2015

(&) Also, referred to as “intra-attention” in Cheng et al., 2016 and some other papers.

Self-Attention

Self-attention, also known as intra-attention, is an attention mechanism relating different

positions of a single sequence in order to compute a representation of the same sequence. It

score(st, hi) = v⊤
a tanh(Wa[st−1; hi])

αt,i = softmax(Wast)

score(st, hi) = s
⊤
t Wahi

Wa

score(st, hi) = s
⊤
t hi

score(st, hi) =
s

⊤
t hi

√n

1/√n

https://arxiv.org/pdf/1409.0473.pdf
https://arxiv.org/pdf/1508.04025.pdf
https://arxiv.org/pdf/1508.04025.pdf
https://arxiv.org/pdf/1508.4025.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
https://arxiv.org/pdf/1601.06733.pdf
http://proceedings.mlr.press/v37/xuc15.pdf
http://proceedings.mlr.press/v37/xuc15.pdf
https://arxiv.org/pdf/1508.04025.pdf


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 8/25

has been shown to be very useful in machine reading, abstractive summarization, or image

description generation.

The long short-term memory network paper used self-attention to do machine reading. In the

example below, the self-attention mechanism enables us to learn the correlation between the

current words and the previous part of the sentence.

Fig. 6. The current word is in red and the size of the blue shade indicates
the activation level. (Image source: Cheng et al., 2016)

Soft vs Hard Attention

In the show, attend and tell paper, attention mechanism is applied to images to generate

captions. The image is first encoded by a CNN to extract features. Then a LSTM decoder

consumes the convolution features to produce descriptive words one by one, where the

weights are learned through attention. The visualization of the attention weights clearly

demonstrates which regions of the image the model is paying attention to so as to output a

certain word.

https://arxiv.org/pdf/1601.06733.pdf
https://arxiv.org/pdf/1601.06733.pdf
http://proceedings.mlr.press/v37/xuc15.pdf


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 9/25

Fig. 7. "A woman is throwing a frisbee in a park." (Image source: Fig. 6(b) in
Xu et al. 2015)

This paper first proposed the distinction between “soft” vs “hard” attention, based on whether

the attention has access to the entire image or only a patch:

Soft Attention: the alignment weights are learned and placed “softly” over all patches in the

source image; essentially the same type of attention as in Bahdanau et al., 2015.

Pro: the model is smooth and differentiable.

Con: expensive when the source input is large.

Hard Attention: only selects one patch of the image to attend to at a time.

Pro: less calculation at the inference time.

Con: the model is non-differentiable and requires more complicated techniques such as

variance reduction or reinforcement learning to train. (Luong, et al., 2015)

Global vs Local Attention

Luong, et al., 2015 proposed the “global” and “local” attention. The global attention is similar to

the soft attention, while the local one is an interesting blend between hard and soft, an

http://proceedings.mlr.press/v37/xuc15.pdf
https://arxiv.org/abs/1409.0473
https://arxiv.org/abs/1508.04025
https://arxiv.org/pdf/1508.04025.pdf


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 10/25

improvement over the hard attention to make it differentiable: the model first predicts a single

aligned position for the current target word and a window centered around the source position

is then used to compute a context vector.

Fig. 8. Global vs local attention (Image source: Fig 2 & 3 in Luong, et al.,
2015)

Neural Turing Machines

Alan Turing in 1936 proposed a minimalistic model of computation. It is composed of a infinitely

long tape and a head to interact with the tape. The tape has countless cells on it, each filled

with a symbol: 0, 1 or blank (" “). The operation head can read symbols, edit symbols and move

left/right on the tape. Theoretic
The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.

4/8/24, 9:10 PM The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.

https://jalammar.github.io/illustrated-transformer/ 1/13

(/)Jay Alammar (/)
Visualizing machine learning one concept at a time.
@JayAlammar (https://twitter.com/JayAlammar) on Twitter. YouTube Channel
(https://www.youtube.com/channel/UCmOwsoHty5PrmE-3QhUBfPQ)

Blog (/) About (/about)

The Illustrated Transformer

In the previous post, we looked at Attention (https://jalammar.github.io/visualizing-neural-machine-translation-
mechanics-of-seq2seq-models-with-attention/) – a ubiquitous method in modern deep learning models. Attention is a
concept that helped improve the performance of neural machine translation applications. In this post, we will look at
The Transformer – a model that uses attention to boost the speed with which these models can be trained. The
Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit,
however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation
to use The Transformer as a reference model to use their Cloud TPU (https://cloud.google.com/tpu/) offering. So let’s
try to break the model apart and look at how it functions.

The Transformer was proposed in the paper Attention is All You Need (https://arxiv.org/abs/1706.03762). A TensorFlow
implementation of it is available as a part of the Tensor2Tensor (https://github.com/tensorflow/tensor2tensor) package.
Harvard’s NLP group created a guide annotating the paper with PyTorch implementation
(http://nlp.seas.harvard.edu/2018/04/03/attention.html). In this post, we will attempt to oversimplify things a bit and
introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of
the subject matter.

2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:

Discussions: Hacker News (65 points, 4 comments) (https://news.ycombinator.com/item?id=18351674), Reddit r/MachineLearning (29 points, 3 comments)

(https://www.reddit.com/r/MachineLearning/comments/8uh2yz/p_the_illustrated_transformer_a_visual_look_at/)

Translations: Arabic (https://www.mundhor.site/post/post14), Chinese (Simplified) 1 (https://blog.csdn.net/yujianmin1990/article/details/85221271), Chinese

(Simplified) 2 (https://blog.csdn.net/qq_36667170/article/details/124359818), French 1 (https://a-coles.github.io/2020/11/15/transformer-illustre.html), French

2 (https://lbourdois.github.io/blog/nlp/Transformer/), Italian (https://medium.com/@val.mannucci/il-transformer-illustrato-it-37a78e3e2348), Japanese

(https://tips-memo.com/translation-jayalmmar-transformer), Korean (https://nlpinkorean.github.io/illustrated-transformer/), Persian

(http://dml.qom.ac.ir/2022/05/17/illustrated-transformer/), Russian (https://habr.com/ru/post/486358/), Spanish 1

(https://www.ibidemgroup.com/edu/transformer-ilustrado-jay-alammar/), Spanish 2 (https://hackernoon.com/el-transformador-ilustrado-una-traduccion-al-

espanol-0y73wwp), Vietnamese (https://trituenhantao.io/tin-tuc/minh-hoa-transformer/)

Watch: MIT’s Deep Learning State of the Art (https://youtu.be/53YvP6gdD7U?t=432) lecture referencing this post

Featured in courses at Stanford (https://web.stanford.edu/class/cs224n/), Harvard (https://scholar.harvard.edu/binxuw/classes/machine-learning-

scratch/materials/transformers), MIT (https://ocw.mit.edu/courses/6-s897-machine-learning-for-healthcare-spring-

2019/d39a6eed387ee90b1f72a01949c35c7b_MIT6_S897S19_lec8.pdf), Princeton (https://www.cs.princeton.edu/courses/archive/fall22/cos597G/), CMU

(https://www.cs.cmu.edu/~leili/course/mldl22w/14-Transformer.pdf) and others

https://jalammar.github.io/
https://jalammar.github.io/
https://jalammar.github.io/
https://twitter.com/JayAlammar
https://twitter.com/JayAlammar
https://www.youtube.com/channel/UCmOwsoHty5PrmE-3QhUBfPQ
https://www.youtube.com/channel/UCmOwsoHty5PrmE-3QhUBfPQ
https://jalammar.github.io/
https://jalammar.github.io/about
https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
https://cloud.google.com/tpu/
https://cloud.google.com/tpu/
https://arxiv.org/abs/1706.03762
https://arxiv.org/abs/1706.03762
https://github.com/tensorflow/tensor2tensor
https://github.com/tensorflow/tensor2tensor
http://nlp.seas.harvard.edu/2018/04/03/attention.html
http://nlp.seas.harvard.edu/2018/04/03/attention.html
https://news.ycombinator.com/item?id=18351674
https://news.ycombinator.com/item?id=18351674
https://www.reddit.com/r/MachineLearning/comments/8uh2yz/p_the_illustrated_transformer_a_visual_look_at/
https://www.reddit.com/r/MachineLearning/comments/8uh2yz/p_the_illustrated_transformer_a_visual_look_at/
https://www.mundhor.site/post/post14
https://www.mundhor.site/post/post14
https://blog.csdn.net/yujianmin1990/article/details/85221271
https://blog.csdn.net/yujianmin1990/article/details/85221271
https://blog.csdn.net/qq_36667170/article/details/124359818
https://blog.csdn.net/qq_36667170/article/details/124359818
https://blog.csdn.net/qq_36667170/article/details/124359818
https://a-coles.github.io/2020/11/15/transformer-illustre.html
https://a-coles.github.io/2020/11/15/transformer-illustre.html
https://lbourdois.github.io/blog/nlp/Transformer/
https://lbourdois.github.io/blog/nlp/Transformer/
https://lbourdois.github.io/blog/nlp/Transformer/
https://medium.com/@val.mannucci/il-transformer-illustrato-it-37a78e3e2348
https://medium.com/@val.mannucci/il-transformer-illustrato-it-37a78e3e2348
https://tips-memo.com/translation-jayalmmar-transformer
https://tips-memo.com/translation-jayalmmar-transformer
https://nlpinkorean.github.io/illustrated-transformer/
https://nlpinkorean.github.io/illustrated-transformer/
http://dml.qom.ac.ir/2022/05/17/illustrated-transformer/
http://dml.qom.ac.ir/2022/05/17/illustrated-transformer/
https://habr.com/ru/post/486358/
https://habr.com/ru/post/486358/
https://www.ibidemgroup.com/edu/transformer-ilustrado-jay-alammar/
https://www.ibidemgroup.com/edu/transformer-ilustrado-jay-alammar/
https://hackernoon.com/el-transformador-ilustrado-una-traduccion-al-espanol-0y73wwp
https://hackernoon.com/el-transformador-ilustrado-una-traduccion-al-espanol-0y73wwp
https://hackernoon.com/el-transformador-ilustrado-una-traduccion-al-espanol-0y73wwp
https://trituenhantao.io/tin-tuc/minh-hoa-transformer/
https://trituenhantao.io/tin-tuc/minh-hoa-transformer/
https://youtu.be/53YvP6gdD7U?t=432
https://youtu.be/53YvP6gdD7U?t=432
https://web.stanford.edu/class/cs224n/
https://web.stanford.edu/class/cs224n/
https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/transformers
https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/transformers
https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/transformers
https://ocw.mit.edu/courses/6-s897-machine-learning-for-healthcare-spring-2019/d39a6eed387ee90b1f72a01949c35c7b_MIT6_S897S19_lec8.pdf
https://ocw.mit.edu/courses/6-s897-machine-learning-for-healthcare-spring-2019/d39a6eed387ee90b1f72a01949c35c7b_MIT6_S897S19_lec8.pdf
https://ocw.mit.edu/courses/6-s897-machine-learning-for-healthcare-spring-2019/d39a6eed387ee90b1f72a01949c35c7b_MIT6_S897S19_lec8.pdf
https://www.cs.princeton.edu/courses/archive/fall22/cos597G/
https://www.cs.princeton.edu/courses/archive/fall22/cos597G/
https://www.cs.cmu.edu/~leili/course/mldl22w/14-Transformer.pdf
https://www.cs.cmu.edu/~leili/course/mldl22w/14-Transformer.pdf


4/8/24, 9:10 PM The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.

https://jalammar.github.io/illustrated-transformer/ 2/13

The Narrated Transformer Language ModelThe Narrated Transformer Language Model

A High-Level Look

Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a
sentence in one language, and output its translation in another.

Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and
connections between them.

The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing
magical about the number six, one can definitely experiment with other arrangements). The decoding component is a
stack of decoders of the same number.

https://www.youtube.com/watch?v=-QH8fRhqFHM


4/8/24, 9:10 PM The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.

https://jalammar.github.io/illustrated-transformer/ 3/13

The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-
layers:

The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the
input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.

The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network
is independently applied to each position.

The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant
parts of the input sentence (similar what attention does in seq2seq models (https://jalammar.github.io/visualizing-
neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)).

https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/


4/8/24, 9:10 PM The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.

https://jalammar.github.io/illustrated-transformer/ 4/13

Bringing The Tensors Into The Picture

Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how
they flow between these components to turn the input of a trained model into an output.

As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding
algorithm (https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca).

Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.

The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that
they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in
other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we
can set – basically it would be the length of the longest sentence in our training dataset.

After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.

https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca
https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca
https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca


4/8/24, 9:10 PM The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.

https://jalammar.github.io/illustrated-transformer/ 5/13

Here we begin to see one key property of the Transformer, which is that the word in each position flows through its
own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward
layer does not have those dependencies, however, and thus the various paths can be executed in parallel while
flowing through the feed-forward layer.

Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the
encoder.

Now We’re Encoding!

As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these
vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the
next encoder.

The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the
exact same network with each vector flowing through it separately.

Self-Attention at a High Level

Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I
had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it
works.

Say the following sentence is an input sentence we want to translate:

”The animal didn't cross the street because it was too tired”

What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human,
but not as simple to an algorithm.

When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.

As the model processes each word (each position in the input sequence), self attention allows it to look at other
positions in the input sequence for clues that can help lead to a better encoding for this word.

If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation
of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the
Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.

As we are encoding the word "it" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on "The
Animal", and baked a part of its representation into the encoding of "it".

Be sure to check out the Tensor2Tensor notebook
(https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)
where you can load a Transformer model, and examine it using this interactive visualization.

https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb
https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb


4/8/24, 9:10 PM The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.

https://jalammar.github.io/illustrated-transformer/ 6/13

Self-Attention in Detail

Let’s first look at how to c
ally a Turing machine can simulate any computer algorithm,

irrespective of how complex or expensive the procedure might be. The infinite memory gives a

Turing machine an edge to be mathematically limitless. However, infinite memory is not feasible

in real modern computers and then we only consider Turing machine as a mathematical model

of computation.

https://arxiv.org/pdf/1508.04025.pdf
https://arxiv.org/pdf/1508.04025.pdf
https://en.wikipedia.org/wiki/Turing_machine


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 11/25

Fig. 9. How a Turing machine looks like: a tape + a head that handles the
tape. (Image source: http://aturingmachine.com/)

Neural Turing Machine (NTM, Graves, Wayne & Danihelka, 2014) is a model architecture for

coupling a neural network with external memory storage. The memory mimics the Turing

machine tape and the neural network controls the operation heads to read from or write to the

tape. However, the memory in NTM is finite, and thus it probably looks more like a “Neural von

Neumann Machine”.

NTM contains two major components, a controller neural network and a memory bank.

Controller: is in charge of executing operations on the memory. It can be any type of neural

network, feed-forward or recurrent. Memory: stores processed information. It is a matrix of size

, containing N vector rows and each has  dimensions.

In one update iteration, the controller processes the input and interacts with the memory bank

accordingly to generate output. The interaction is handled by a set of parallel read and write

heads. Both read and write operations are “blurry” by softly attending to all the memory

addresses.

N × M M

http://aturingmachine.com/
https://arxiv.org/abs/1410.5401
https://en.wikipedia.org/wiki/Von_Neumann_architecture
https://en.wikipedia.org/wiki/Von_Neumann_architecture


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 12/25

Fig 10. Neural Turing Machine Architecture.

Reading and Writing

When reading from the memory at time t, an attention vector of size ,  controls how much

attention to assign to different memory locations (matrix rows). The read vector  is a sum

weighted by attention intensity:

where  is the -th element in  and  is the -th row vector in the memory.

When writing into the memory at time t, as inspired by the input and forget gates in LSTM, a

write head first wipes off some old content according to an erase vector  and then adds new

information by an add vector .

Attention Mechanisms

In Neural Turing Machine, how to generate the attention distribution  depends on the

addressing mechanisms: NTM uses a mixture of content-based and location-based

addressings.

N wt

rt

rt =
N

∑
i=1

wt(i)Mt(i), where 
N

∑
i=1

wt(i) = 1, ∀i : 0 ≤ wt(i) ≤ 1

wt(i) i wt Mt(i) i

et

at

~
Mt(i) = Mt−1(i)[1 − wt(i)et] ; erase

Mt(i) =
~

Mt(i) + wt(i)at ; add

wt



4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 13/25

Content-based addressing

The content-addressing creates attention vectors based on the similarity between the key

vector  extracted by the controller from the input and memory rows. The content-based

attention scores are computed as cosine similarity and then normalized by softmax. In addition,

NTM adds a strength multiplier  to amplify or attenuate the focus of the distribution.

Interpolation

Then an interpolation gate scalar  is used to blend the newly generated content-based

attention vector with the attention weights in the last time step:

Location-based addressing

The location-based addressing sums up the values at different positions in the attention vector,

weighted by a weighting distribution over allowable integer shifts. It is equivalent to a 1-d

convolution with a kernel , a function of the position offset. There are multiple ways to

define this distribution. See Fig. 11. for inspiration.

Fig. 11. Two ways to represent the shift weighting distribution .

Finally the attention distribution is enhanced by a sharpening scalar .

kt

βt

wc
t(i) = softmax(βt ⋅ cosine[kt, Mt(i)]) =

exp(βt
kt⋅Mt(i)

∥kt∥⋅∥Mt(i)∥ )

∑N
j=1 exp(βt

kt⋅Mt(j)
∥kt∥⋅∥Mt(j)∥ )

gt

w
g
t = gtw

c
t + (1 − gt)wt−1

st(. )

s_t

γt ≥ 1



4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 14/25

The complete process of generating the attention vector  at time step t is illustrated in Fig.

12. All the parameters produced by the controller are unique for each head. If there are multiple

read and write heads in parallel, the controller would output multiple sets.

Fig. 12. Flow diagram of the addressing mechanisms in Neural Turing
Machine. (Image source: Graves, Wayne & Danihelka, 2014)

Pointer Network

In problems like sorting or travelling salesman, both input and output are sequential data.

Unfortunately, they cannot be easily solved by classic seq-2-seq or NMT models, given that the

discrete categories of output elements are not determined in advance, but depends on the

variable input size. The Pointer Net (Ptr-Net; Vinyals, et al. 2015) is proposed to resolve this

type of problems: When the output elements correspond to positions in an input sequence.

Rather than using attention to blend hidden units of an encoder into a context vector (See Fig.

8), the Pointer Net applies attention over the input elements to pick one as the output at each

decoder step.

~wt(i) =
N

∑
j=1

w
g
t (j)st(i − j) ; circular convolution

wt(i) =
~wt(i)γt

∑N
j=1

~wt(j)γt
; sharpen

wt

https://arxiv.org/abs/1410.5401
https://arxiv.org/abs/1506.03134


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 15/25

Fig. 13. The architecture of a Pointer Network model. (Image source:
Vinyals, et al. 2015)

The Ptr-Net outputs a sequence of integer indices,  given a sequence of input

vectors  and . The model still embraces an encoder-decoder

framework. The encoder and decoder hidden states are denoted as  and

, respectively. Note that  is the output gate after cell activation in the decoder.

The Ptr-Net applies additive attention between states and then normalizes it by softmax to

model the output conditional probability:

The attention mechanism is simplified, as Ptr-Net does not blend the encoder states into the

output with attention weights. In this way, the output only responds to the positions but not the

input content.

Transformer

“Attention is All you Need” (Vaswani, et al., 2017), without a doubt, is one of the most impactful

and interesting paper in 2017. It presented a lot of improvements to the soft attention and make

it possible to do seq2seq modeling without recurrent network units. The proposed

c = (c1, … , cm)

x = (x1, … ,xn) 1 ≤ ci ≤ n

(h1, … , hn)
(s1, … , sm) si

yi = p(ci|c1, … , ci−1, x)

= softmax(score(st; hi)) = softmax(v⊤
a tanh(Wa[st; hi]))

https://arxiv.org/abs/1506.03134
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 16/25

“transformer” model is entirely built on the self-attention mechanisms without using

sequence-aligned recurrent architecture.

The secret recipe is carried in its model architecture.

Key, Value and Query

The major component in the transformer is the unit of multi-head self-attention mechanism.

The transformer views the encoded representation of the input as a set of key-value pairs,

, both of dimension  (input sequence length); in the context of NMT, both the keys and

values are the encoder hidden states. In the decoder, the previous output is compressed into a

query (  of dimension ) and the next output is produced by mapping this query and the set

of keys and values.

The transformer adopts the scaled dot-product attention: the output is a weighted sum of the

values, where the weight assigned to each value is determined by the dot-product of the query

with all the keys:

Multi-Head Self-Attention

Fig. 14. Multi-head scaled dot-product attention mechanism. (Image
source: Fig 2 in Vaswani, et al., 2017)

(K, V) n

Q m

Attention(Q, K, V) = softmax(
QK⊤

√n
)V

http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 17/25

Rather than only computing the attention once, the multi-head mechanism runs through the

scaled dot-product attention multiple times in parallel. The independent attention outputs are

simply concatenated and linearly transformed into the expected dimensions. I assume the

motivation is because ensembling always helps? ;) According to the paper, “multi-head

attention allows the model to jointly attend to information from different representation

subspaces at different positions. With a single attention head, averaging inhibits this.”

where , , , and  are parameter matrices to be learned.

Encoder

Fig. 15. The transformer s̓ encoder. (Image source: Vaswani, et al., 2017)

The encoder generates an attention-based representation with capability to locate a specific

piece of information from a potentially infinitely-large context.

A stack of N=6 identical layers.

Each layer has a multi-head self-attention layer and a simple position-wise fully

connected feed-forward network.

Each sub-layer adopts a residual connection and a layer normalization. All the sub-layers

output data of the same dimension .

Decoder

MultiHead(Q, K, V) = [head1; … ; headh]WO

where headi = Attention(QW
Q
i , KWK

i , VWV
i )

W
Q
i WK

i WV
i WO

dmodel = 512

http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
https://arxiv.org/pdf/1512.03385.pdf


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 18/25

Fig. 16. The transformer s̓ decoder. (Image source: Vaswani, et al., 2017)

The decoder is able to retrieval from the encoded representation.

A stack of N = 6 identical layers

Each layer has two sub-layers of multi-head attention mechanisms and one sub-layer of

fully-connected feed-forward network.

Similar to the encoder, each sub-layer adopts a residual connection and a layer

normalization.

The first multi-head attention sub-layer is modified to prevent positions from attending to

subsequent positions, as we donʼt want to look into the future of the target sequence when

predicting the current position.

Full Architecture

Finally here is the complete view of the transformer s̓ architecture:

Both the source and target sequences first go through embedding layers to produce data of

the same dimension .

To preserve the position information, a sinusoid-wave-based positional encoding is applied

and summed with the embedding output.

A softmax and linear layer are added to the final decoder output.

dmodel = 512

http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 19/25

Fig. 17. The full model architecture of the transformer. (Image source: Fig 1
& 2 in Vaswani, et al., 2017.)

Try to implement the transformer model is an interesting experience, here is mine:

lilianweng/transformer-tensorflow. Read the comments in the code if you are interested.

SNAIL

The transformer has no recurrent or convolutional structure, even with the positional encoding

added to the embedding vector, the sequential order is only weakly incorporated. For problems

sensitive to the positional dependency like reinforcement learning, this can be a big problem.

The Simple Neural Attention Meta-Learner (SNAIL) (Mishra et al., 2017) was developed

partially to resolve the problem with positioning in the transformer model by combining the self-

attention mechanism in transformer with temporal convolutions. It has been demonstrated to be

good at both supervised learning and reinforcement learning tasks.

http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
https://github.com/lilianweng/transformer-tensorflow
https://lilianweng.github.io/posts/2018-02-19-rl-overview/
http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/
http://metalearning.ml/papers/metalearn17_mishra.pdf
https://deepmind.com/blog/wavenet-generative-model-raw-audio/


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 20/25

Fig. 18. SNAIL model architecture (Image source: Mishra et al., 2017)

SNAIL was born in the field of meta-learning, which is another big topic worthy of a post by

itself. But in simple words, the meta-learning model is expected to be generalizable to novel,

unseen tasks in the similar distribution. Read this nice introduction if interested.

Self-Attention GAN

Self-Attention GAN (SAGAN; Zhang et al., 2018) adds self-attention layers into GAN to enable

both the generator and the discriminator to better model relationships between spatial regions.

The classic DCGAN (Deep Convolutional GAN) represents both discriminator and generator as

multi-layer convolutional networks. However, the representation capacity of the network is

restrained by the filter size, as the feature of one pixel is limited to a small local region. In order

to connect regions far apart, the features have to be dilute through layers of convolutional

operations and the dependencies are not guaranteed to be maintained.

As the (soft) self-attention in the vision context is designed to explicitly learn the relationship

between one pixel and all other positions, even regions far apart, it can easily capture global

dependencies. Hence GAN equipped with self-attention is expected to handle details better,

hooray!

http://metalearning.ml/papers/metalearn17_mishra.pdf
http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/
https://arxiv.org/pdf/1805.08318.pdf
https://lilianweng.github.io/posts/2017-08-20-gan/
https://arxiv.org/abs/1511.06434


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 21/25

Fig. 19. Convolution operation and self-attention have access to regions of
very different sizes.

The SAGAN adopts the non-local neural network to apply the attention computation. The

convolutional image feature maps  is branched out into three copies, corresponding to the

concepts of key, value, and query in the transformer:

Key: 

Query: 

Value: 

Then we apply the dot-product attention to output the self-attention feature maps:

x

f(x) = Wfx

g(x) = Wgx

h(x) = Whx

αi,j = softmax(f(xi)
⊤g(xj))

oj = Wv(
N

∑
i=1

αi,jh(xi))

https://arxiv.org/pdf/1711.07971.pdf


4/8/24, 9:10 PM Attention? Attention! | Lil'Log

https://lilianweng.github.io/posts/2018-06-24-attention/ 22/25

Fig. 20. The self-attention mechanism in SAGAN. (Image so

##Your Answer:
